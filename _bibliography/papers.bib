---
---

@article{bao2024faithbench,
  abbr = {arXiv},
  title={FaithBench: A Diverse Hallucination Benchmark for Summarization by Modern LLMs},
  author={Bao, Forrest Sheng and Li, Miaoran and Qu, Renyi and Luo, Ge and Wan, Erana and Tang, Yujia and Fan, Weisi and Tamber, Manveer Singh and Kazi, Suleman and Sourabh, Vivek and others},
  eprint={arXiv:2410.13210},
  year={2024},
  archivePrefix={arXiv},
  primaryClass={cs.IR},
  url={https://arxiv.org/abs/2410.13210},
  pdf={https://arxiv.org/abs/2410.13210.pdf},
  preview={faithbench.png},
  selected={false},
}

@article{tamber2024can,
  abbr = {arXiv},
  title={Can't Hide Behind the API: Stealing Black-Box Commercial Embedding Models},
  author={Tamber, Manveer Singh and Xian, Jasper and Lin, Jimmy},
  eprint={arXiv:2406.09355},
  year={2024},
  archivePrefix={arXiv},
  primaryClass={cs.IR},
  url={https://arxiv.org/abs/2406.09355},
  pdf={https://arxiv.org/abs/2406.09355.pdf},
  selected={true},
}

@article{tamber2023scalingdownlittingup,
  abbr = {arXiv},
  title={Scaling Down, LiTting Up: Efficient Zero-Shot Listwise Reranking with Seq2seq Encoder-Decoder Models}, 
  author={Manveer Singh Tamber and Ronak Pradeep and Jimmy Lin},
  year={2023},
  eprint={2312.16098},
  archivePrefix={arXiv},
  primaryClass={cs.IR},
  url={https://arxiv.org/abs/2312.16098},
  pdf={https://arxiv.org/abs/2312.16098.pdf},
  preview={lit_architecture.png},
  selected={false},
}

@InProceedings{10.1007/978-3-031-28241-6_11,
abbr = {ECIR},
author="Tamber, Manveer Singh
and Pradeep, Ronak
and Lin, Jimmy",
editor="Kamps, Jaap
and Goeuriot, Lorraine
and Crestani, Fabio
and Maistro, Maria
and Joho, Hideo
and Davis, Brian
and Gurrin, Cathal
and Kruschwitz, Udo
and Caputo, Annalina",
title="Pre-processing Matters! Improved Wikipedia Corpora for Open-Domain Question Answering",
booktitle="Advances in Information Retrieval",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="163--176",
abstract="One of the contributions of the landmark Dense Passage Retriever (DPR) work is the curation of a corpus of passages generated from Wikipedia articles that have been segmented into non-overlapping passages of 100 words. This corpus has served as the standard source for question answering systems based on a retriever--reader pipeline and provides the basis for nearly all state-of-the-art results on popular open-domain question answering datasets. There are, however, multiple potential drawbacks to this corpus. First, the passages do not include tables, infoboxes, and lists. Second, the choice to split articles into non-overlapping passages results in fragmented sentences and disjoint passages that models might find hard to reason over. In this work, we experimented with multiple corpus variants from the same Wikipedia source, differing in passage size, overlapping passages, and the inclusion of linearized semi-structured data. The main contribution of our work is the replication of Dense Passage Retriever and Fusion-in-Decoder training on our corpus variants, allowing us to validate many of the findings in previous work and giving us new insights into the importance of corpus pre-processing for open-domain question answering. With better data preparation, we see improvements of over one point on both the Natural Questions dataset and the TriviaQA dataset in end-to-end effectiveness over previous work measured using the exact match score. Our results demonstrate the importance of careful corpus curation and provide the basis for future work.",
isbn="978-3-031-28241-6",
}


@InProceedings{10.1007/978-3-031-28241-6_10,
abbr = {ECIR},
author="Pradeep, Ronak
and Chen, Haonan
and Gu, Lingwei
and Tamber, Manveer Singh
and Lin, Jimmy",
editor="Kamps, Jaap
and Goeuriot, Lorraine
and Crestani, Fabio
and Maistro, Maria
and Joho, Hideo
and Davis, Brian
and Gurrin, Cathal
and Kruschwitz, Udo
and Caputo, Annalina",
title="PyGaggle: A Gaggle of Resources for Open-Domain Question Answering",
booktitle="Advances in Information Retrieval",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="148--162",
abstract="Text retrieval using dense--sparse hybrids has been gaining popularity because of their effectiveness. Improvements to both sparse and dense models have also been noted, in the context of open-domain question answering. However, the increasing sophistication of proposed techniques places a growing strain on the reproducibility of results. Our work aims to tackle this challenge. In Generation Augmented Retrieval (GAR), a sequence-to-sequence model was used to generate candidate answer strings as well as titles of documents and actual sentences where the answer string might appear; this query expansion was applied before traditional sparse retrieval. Distilling Knowledge from Reader to Retriever (DKRR) used signals from downstream tasks to train a more effective Dense Passage Retrieval (DPR) model. In this work, we first replicate the results of GAR using a different codebase and leveraging a more powerful sequence-to-sequence model, T5. We provide tight integration with Pyserini, a popular IR toolkit, where we also add support for the DKRR-based DPR model: the combination demonstrates state-of-the-art effectiveness for retrieval in open-domain QA. To account for progress in generative readers that leverage evidence fusion for QA, so-called fusion-in-decoder (FiD), we incorporate these models into our PyGaggle toolkit. The result is a reproducible, easy-to-use, and powerful end-to-end question-answering system that forms a starting point for future work. Finally, we provide evaluation tools that better gauge whether models are generalizing or simply memorizing.",
isbn="978-3-031-28241-6"
}



@ARTICLE{9674865,
  abbr = {IEEE JSTARS},
  author={Tamber, Manveer Singh and Scott, K. Andrea and Pedersen, Leif Toudal},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 
  title={Accounting for Label Errors When Training a Convolutional Neural Network to Estimate Sea Ice Concentration Using Operational Ice Charts}, 
  year={2022},
  volume={15},
  number={},
  pages={1502-1513},
  keywords={Sea ice;Synthetic aperture radar;Radar polarimetry;Training;Convolutional neural networks;Predictive models;Uncertainty;Convolutional neural network (CNN);ice concentration;synthetic aperture radar (SAR) data},
  doi={10.1109/JSTARS.2022.3141063},
  preview={accounting_ice.png},
  }


@INPROCEEDINGS{9518875,
  abbr = {IEEE ANTEM},
  author={Andrea Scott, K. and Tamber, Manveer},
  booktitle={2021 IEEE 19th International Symposium on Antenna Technology and Applied Electromagnetics (ANTEM)}, 
  title={Experiments with estimation of sea ice concentration using a convolutional neural network and microwave data}, 
  year={2021},
  volume={},
  number={},
  pages={1-2},
  keywords={Microwave integrated circuits;Silicon carbide;Estimation;Predictive models;Tools;Convolutional neural networks;Microwave FET integrated circuits;sea ice;synthetic aperture radar;convolutional neural network},
  doi={10.1109/ANTEM51107.2021.9518875},
  preview={experiments_ice.png},
  }
